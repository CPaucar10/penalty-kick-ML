WEBVTT

1
00:00:02.760 --> 00:00:11.830
Christopher Paucar: Hello! I hope this finds you well, and today I'm going to be present to you the word that I've been doing in this. I. W. Seminar, Ted up predicting penalty, kicks Demo Models

2
00:00:12.790 --> 00:00:23.340
Christopher Paucar: Football also on the soccer is the world's most popular sport. 2 teams have 11 play against each other, and if it's time we go to an extra 30, and if it's still tied, we go to a penalty, kick shootout

3
00:00:23.390 --> 00:00:33.490
Christopher Paucar: where the kicker of one team has to shoot the ball past the goalkeeper into the net of the other team and the winner of the shootout when the entire game.

4
00:00:33.600 --> 00:00:48.830
Christopher Paucar: and because of this penalty Kicks actually have a big role in deciding champions. You score one, you win the World Cup. Are you be the best sport, the best trophy and all the sport hail as a hero, or you miss scapegoated, and in this case

5
00:00:49.810 --> 00:00:54.360
Christopher Paucar: abused by fans around the world, which is totally unacceptable.

6
00:00:55.210 --> 00:00:59.330
Christopher Paucar: I've been on both sides. I've scored penalties, and I've sometimes missed them.

7
00:00:59.600 --> 00:01:04.230
Christopher Paucar: And I've on TV Wash them live trying to predict if they'll go in or not.

8
00:01:04.480 --> 00:01:06.580
Christopher Paucar: But what if there is a way to have a model?

9
00:01:06.780 --> 00:01:10.280
Christopher Paucar: Predict it for me? What if there's a way for model to predict the scores?

10
00:01:10.340 --> 00:01:18.070
Christopher Paucar: And compared against my own predictions and compared those with what actually happened. And so that's my goal.

11
00:01:18.340 --> 00:01:33.330
Christopher Paucar: Using different mo models, I want to be able to offer the ability to predict whether penalty kick will go in or not, based on certain features. Some previous work I want to highlight on the left. We have robot keeper, which is a robotic goalkeeper that uses image processing

12
00:01:33.540 --> 00:01:35.500
Christopher Paucar: and sensors on the ball.

13
00:01:35.570 --> 00:01:38.250
Christopher Paucar: See how to predict where the ball will go.

14
00:01:38.330 --> 00:01:42.980
Christopher Paucar: using dynamics and planning. Have the robot dive left, right up or down

15
00:01:43.800 --> 00:01:51.440
Christopher Paucar: on the right side. We have this model, this predictor by lot, Brandon and Jesse Davis that uses in-game metrics

16
00:01:51.920 --> 00:01:58.620
Christopher Paucar: to be able to predict whether a penalty kick will go, such as the Miss Shots of the shooter, the past accuracy of the Kicker.

17
00:01:59.260 --> 00:02:11.710
Christopher Paucar: whereas in my case, I actually use data points from data sets which transitions into the steps that I use in this project. First, we're scattering data from data sets from

18
00:02:11.720 --> 00:02:18.600
Christopher Paucar: the world top data set from 1,984 to 2,018, and the Primary League data set from 2,016 to 17.

19
00:02:18.730 --> 00:02:25.870
Christopher Paucar: And then the third data set, which consists of the sub data sets a pounding is from Spain. La Liga, France, like on

20
00:02:26.250 --> 00:02:27.470
Christopher Paucar: among others.

21
00:02:28.290 --> 00:02:34.540
Christopher Paucar: and particularly I want to highlight these in both the missed penalty case. We will come in handy later on

22
00:02:34.660 --> 00:02:46.610
Christopher Paucar: the pan. This Python library was very helpful in that, and that was feature engineering where I did feature extraction. Eda Random oversampling 100 coding, where again the Pandas library was essential.

23
00:02:47.510 --> 00:02:56.020
Christopher Paucar: Third, I did binary classification. We're getting my models to be able to predict whether pounding it will go in or not with sk learn, and I am, you learn.

24
00:02:56.140 --> 00:02:58.090
Christopher Paucar: being helpful Python libraries.

25
00:02:58.420 --> 00:03:04.510
Christopher Paucar: and finally the iterative evaluation, the model performance. They go to score the goal of completing my Iw

26
00:03:04.690 --> 00:03:12.440
Christopher Paucar: precision, accuracy, F. One squared roc with the sk learned seaboard and map of live being helpful in visualizing

27
00:03:12.750 --> 00:03:14.110
Christopher Paucar: these results.

28
00:03:14.380 --> 00:03:22.150
Christopher Paucar: First, I got a data from data sets. Some of them were very clean. I just use them, extract them. No problem. Some of them were a little more cleaning

29
00:03:22.180 --> 00:03:33.890
Christopher Paucar: with the zones, for example, having to translate that into the left center or right. Other times I have to scour through Youtube, such as that. Such and what can be seen in the dark and cell basically jotting down, whether a pound to keep winning or not.

30
00:03:35.280 --> 00:03:44.570
Christopher Paucar: with my data set now chosen. Now I use feature engineering where I decided to extract these features. Dominant foot take a direction, and the keeper direction

31
00:03:44.810 --> 00:03:50.270
Christopher Paucar: with the labels. I wanted to predict being 0, whether a penalty, miss, or one a penalty goal.

32
00:03:51.520 --> 00:04:03.910
Christopher Paucar: Next I perform one encoding, to be able to more accurately spread out the data and expand the number of features from 3 into 8 features, leaving me with a data set of 5, 28 by 9,

33
00:04:03.920 --> 00:04:06.160
Christopher Paucar: 8 features, plus the target.

34
00:04:06.780 --> 00:04:15.550
Christopher Paucar: However, something I did not take into account was that my data wasn't balanced Originally it was 70, 30, 70% going in 30% not.

35
00:04:16.070 --> 00:04:22.130
Christopher Paucar: And this is where the missed penalty data stick came into play because I was able to equalize these 2

36
00:04:22.280 --> 00:04:28.910
Christopher Paucar: labels more closer together with a 60% going in 40%, not going

37
00:04:29.470 --> 00:04:37.590
Christopher Paucar: next for the models I decided to use. I decided to use these following models after performing right over for sampling.

38
00:04:38.100 --> 00:04:43.330
Christopher Paucar: then I use base models which did not require hyper parameter tuning

39
00:04:43.380 --> 00:04:54.290
Christopher Paucar: and compared it with grid search type of a parameter tuning, and then I use balance that classifier from I and B Learn and I decided to use a decision tree model in this case.

40
00:04:56.010 --> 00:05:10.590
Christopher Paucar: Now I want to go over my evaluation results. On the left hand side. You have the F one at Roc scores. I decided not to use accuracy, since my data was in balance on the left side. You have base on the right hand side. You have, after hyper parameter tuning, as you could see.

41
00:05:11.620 --> 00:05:13.950
Christopher Paucar: according to these metrics, very similar.

42
00:05:14.530 --> 00:05:29.260
Christopher Paucar: So I just had to look at the confusion matrices. Here we have base decision, tree, random, forest, and the gradient boosting, and, as you can see, they were all identical, producing 20 times truly, that I went in and actually did for a due test that it did not go with, and it actually did not go in.

43
00:05:29.620 --> 00:05:43.820
Christopher Paucar: Let's compare that with the Svm. And the logistic regression. Yes, Vm. Did not perform as well only 10 times compared to 28. No. that's not good enough. Logistic regression was definitely better than Svm. But not to the same extent as it was to the decision tree.

44
00:05:44.960 --> 00:05:55.580
Christopher Paucar: Now let's compare this to the confusion Matrices of these 2 models with the on the left hand side after tuning with random forest decision tree

45
00:05:56.250 --> 00:06:02.570
Christopher Paucar: identical 28 times a predict that it won it, and did 42 tests it. Did not go in and it didn't go in

46
00:06:02.640 --> 00:06:14.040
Christopher Paucar: for the Svm. Actually did improve to 23 as compared to 10 before. but for the logistic regression that ended up being the same, and actually for great and to boosting, it actually did a bit worse.

47
00:06:15.150 --> 00:06:25.200
Christopher Paucar: so I decided to scratch the Svm. The regression and the gradient boosting. and because they were identical, I decided to go with the decision tree.

48
00:06:26.460 --> 00:06:34.190
Christopher Paucar: Now I want to take a look at the roc curves on the left hand side. You have the decision tree, the base, and on the right hand side you have the logistic regression, for by base.

49
00:06:34.600 --> 00:06:44.850
Christopher Paucar: as you can see, the closer this is the top left-hand corner, the better and the base decision tree. Our secrets follow that whereas on the logistic side, since it very.

50
00:06:44.870 --> 00:06:51.560
Christopher Paucar: it follows close to the white with X. It's no better than it's guessing. So I just had to go with the base decision Tree

51
00:06:55.030 --> 00:07:08.040
Christopher Paucar: now results from my balance. That classifier, as you can see, the the the confusion matrix was actually pretty much the same. 28 times it went in, and it did go in, and it it did not go in, and it really didn't go in.

52
00:07:08.140 --> 00:07:11.970
Christopher Paucar: and the Rc. Curve for the balance that was very similar.

53
00:07:12.350 --> 00:07:13.670
So, as you can see.

54
00:07:14.150 --> 00:07:22.450
Christopher Paucar: even after performing hyper parameter tuning and using a different type of model specifically made for in balanced data sets.

55
00:07:23.340 --> 00:07:32.190
Christopher Paucar: which is now on my conclusion, I was at a stampede in terms of performance F one for that based decision. Tree was point 7. Roc score was point 6, 7,

56
00:07:32.350 --> 00:07:35.420
and that surprised me. I didn't expect to

57
00:07:35.550 --> 00:07:45.500
Christopher Paucar: be looking for more deciding factors and predicting penalty kicks. So, following the advice from Professor Lee, I intend to improve my performance by extracting more features.

58
00:07:46.860 --> 00:07:54.700
Christopher Paucar: So there are more features that anticipated to be able to predict these penalty case, something that to me was a little more surprising.

59
00:07:54.780 --> 00:08:12.370
Christopher Paucar: Some future work that I want to be able to conduct was using computer vision. In addition to looking for where the ball is going to go, which is what robot keeper did. Being able to look at the eyes of the penalty could take the head. Maybe the different body angles that the person is making as they about to kick the penalty.

60
00:08:13.960 --> 00:08:23.540
Christopher Paucar: And this is an example getting these much more specific angles and using sensors to be able to predict where the penalty cake is gonna end up being

61
00:08:25.760 --> 00:08:38.130
Christopher Paucar: last. But not least, I do want to give a big thanks to Professor and me, Patel, for the helpful advice throughout the semester to my classmates, and particularly Camille and Natalie, for the advice and the fun times we have throughout the semester.

62
00:08:38.539 --> 00:08:39.440
Christopher Paucar: Thank you.

